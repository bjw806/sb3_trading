{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-04-25T20:45:07.496976Z",
     "iopub.status.busy": "2023-04-25T20:45:07.496697Z",
     "iopub.status.idle": "2023-04-25T20:45:08.612440Z",
     "shell.execute_reply": "2023-04-25T20:45:08.611784Z",
     "shell.execute_reply.started": "2023-04-25T20:45:07.496952Z"
    }
   },
   "outputs": [],
   "source": [
    "import shutil, os\n",
    "if os.path.isdir('rainbow') : shutil.rmtree(\"rainbow\")\n",
    "!git clone https://github.com/ClementPerroud/Rainbow-Agent rainbow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-04-25T20:45:24.690958Z",
     "iopub.status.busy": "2023-04-25T20:45:24.690789Z",
     "iopub.status.idle": "2023-04-25T20:45:27.647431Z",
     "shell.execute_reply": "2023-04-25T20:45:27.646692Z",
     "shell.execute_reply.started": "2023-04-25T20:45:24.690939Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import gymnasium as gym\n",
    "from sklearn.preprocessing import robust_scale\n",
    "\n",
    "from rainbow.agent import Rainbow\n",
    "from preprocess import preprocess\n",
    "\n",
    "import sys\n",
    "import gym_trading_env\n",
    "import nest_asyncio"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create environments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-04-25T20:45:27.649230Z",
     "iopub.status.busy": "2023-04-25T20:45:27.648870Z",
     "iopub.status.idle": "2023-04-25T20:45:28.576145Z",
     "shell.execute_reply": "2023-04-25T20:45:28.575522Z",
     "shell.execute_reply.started": "2023-04-25T20:45:27.649211Z"
    }
   },
   "outputs": [],
   "source": [
    "def add_features(df):\n",
    "    df[\"feature_close\"] = robust_scale(df[\"close\"].pct_change())\n",
    "    df[\"feature_open\"] = robust_scale(df[\"open\"]/df[\"close\"])\n",
    "    df[\"feature_high\"] = robust_scale(df[\"high\"]/df[\"close\"])\n",
    "    df[\"feature_low\"] = robust_scale(df[\"low\"]/df[\"close\"])\n",
    "    df[\"feature_volume\"] = robust_scale(df[\"volume\"] / df[\"volume\"].rolling(7*24).max())\n",
    "    df.dropna(inplace= True)\n",
    "    return df\n",
    "\n",
    "\n",
    "def reward_function(history):\n",
    "    return np.log(history[\"portfolio_valuation\", -1] / history[\"portfolio_valuation\", -2])\n",
    "\n",
    "def max_drawdown(history):\n",
    "    networth_array = history['portfolio_valuation']\n",
    "    _max_networth = networth_array[0]\n",
    "    _max_drawdown = 0\n",
    "    for networth in networth_array:\n",
    "        if networth > _max_networth:\n",
    "            _max_networth = networth\n",
    "        drawdown = ( networth - _max_networth ) / _max_networth\n",
    "        if drawdown < _max_drawdown:\n",
    "            _max_drawdown = drawdown\n",
    "    return f\"{_max_drawdown*100:5.2f}%\"\n",
    "\n",
    "def make_env(dir):\n",
    "    env = gym.make(\n",
    "        \"MultiDatasetTradingEnv\",\n",
    "        dataset_dir= dir,\n",
    "        preprocess= preprocess,\n",
    "        windows= 15,\n",
    "        positions = [ -1, -0.5, 0, 1, 2],\n",
    "        initial_position = 0,\n",
    "        trading_fees = 0.01/100,\n",
    "        borrow_interest_rate= 0.0003/100,\n",
    "        reward_function = reward_function,\n",
    "        portfolio_initial_value = 1000,\n",
    "        verbose=1,\n",
    "    )\n",
    "    env.unwrapped.add_metric('Position Changes', lambda history : f\"{ 100*np.sum(np.diff(history['position']) != 0)/len(history['position']):5.2f}%\" )\n",
    "    env.unwrapped.add_metric('Max Drawdown', max_drawdown)\n",
    "    return env\n",
    "\n",
    "training_envs = gym.vector.SyncVectorEnv([lambda: make_env(\"./data/train/month/**/*.pkl\") for _ in range(5)])\n",
    "validation_envs = gym.vector.SyncVectorEnv([lambda: make_env(\"./data/test/month/2023/*.pkl\") for _ in range(5)])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-04-25T20:45:28.577112Z",
     "iopub.status.busy": "2023-04-25T20:45:28.576930Z",
     "iopub.status.idle": "2023-04-25T20:45:31.067018Z",
     "shell.execute_reply": "2023-04-25T20:45:31.066482Z",
     "shell.execute_reply.started": "2023-04-25T20:45:28.577095Z"
    }
   },
   "outputs": [],
   "source": [
    "agent = Rainbow(\n",
    "    simultaneous_training_env = 5,\n",
    "    \n",
    "    #Distributional\n",
    "    distributional= True,\n",
    "    v_min= -200,\n",
    "    v_max = 250,\n",
    "    nb_atoms= 51, \n",
    "    # Prioritized Replay\n",
    "    prioritized_replay = False,\n",
    "    prioritized_replay_alpha= 0.5,\n",
    "    prioritized_replay_beta_function = lambda episode, step : min(1, 0.5 + 0.5*step/150_000),\n",
    "    \n",
    "    # General\n",
    "    multi_steps = 3,\n",
    "    nb_states = 14,\n",
    "    nb_actions = 4,\n",
    "    gamma = 0.99,\n",
    "    replay_capacity = 1E8,\n",
    "    tau = 2000,\n",
    "    \n",
    "    # Model\n",
    "    window= 15,\n",
    "    units = [16,16, 16],\n",
    "    dropout= 0.2,\n",
    "    adversarial= True,\n",
    "    noisy= False,\n",
    "    learning_rate = 3*2.5E-4,\n",
    "\n",
    "    batch_size= 128,\n",
    "    train_every = 10,\n",
    "    epsilon_function = lambda episode, step : max(0.001, (1 - 5E-5)** step),\n",
    "    name = \"Rainbow\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-04-25T20:45:31.068198Z",
     "iopub.status.busy": "2023-04-25T20:45:31.067808Z",
     "iopub.status.idle": "2023-04-25T20:45:31.072860Z",
     "shell.execute_reply": "2023-04-25T20:45:31.072377Z",
     "shell.execute_reply.started": "2023-04-25T20:45:31.068180Z"
    }
   },
   "outputs": [],
   "source": [
    "def train(steps = 100_000):\n",
    "    print(\"___________________________________________ TRAINING ___________________________________________\")\n",
    "    if 'obs' not in globals():\n",
    "        global obs\n",
    "        obs, info = training_envs.reset()\n",
    "    for _ in range(steps):\n",
    "        actions = agent.e_greedy_pick_actions(obs)\n",
    "        next_obs, rewards, dones, truncateds, infos = training_envs.step(actions)\n",
    "\n",
    "        agent.store_replays(obs, actions, rewards, next_obs, dones, truncateds)\n",
    "        agent.train()\n",
    "\n",
    "        obs = next_obs\n",
    "\n",
    "def evaluation():\n",
    "    print(\"___________________________________________ VALIDATION ___________________________________________\")\n",
    "    val_obs, info = validation_envs.reset()\n",
    "    check = np.array([False for _ in range(val_obs.shape[0])])\n",
    "    while not np.all(check):\n",
    "        actions = agent.e_greedy_pick_actions(val_obs)\n",
    "        next_obs, rewards, dones, truncateds, infos = validation_envs.step(actions)\n",
    "        val_obs = next_obs\n",
    "        check += dones + truncateds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-04-25T20:45:31.073670Z",
     "iopub.status.busy": "2023-04-25T20:45:31.073517Z"
    }
   },
   "outputs": [],
   "source": [
    "while True:\n",
    "    train(steps = 30_000)\n",
    "    evaluation()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dill, pickle\n",
    "#agent.model = None\n",
    "#agent.target_model = None\n",
    "#agent.replay_memory = None\n",
    "\n",
    "with open(\"test.pkl\", \"wb\") as file:\n",
    "    dill.dump(agent, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_indexes, states, actions, rewards, states_prime, dones, importance_weights = agent.replay_memory.sample(\n",
    "    256,\n",
    "    agent.prioritized_replay_beta_function(agent.episode_count, agent.steps)\n",
    ")\n",
    "results = agent.model(states)\n",
    "\n",
    "action_colors=[\"blue\", \"orange\",\"purple\",\"red\"]\n",
    "fig, axes = plt.subplots(nrows = 2, ncols = 2, figsize=(16,9), dpi=300)\n",
    "for action in range(4):\n",
    "    for i in range(256):\n",
    "        axes[action%2, action//2%2].plot(agent.zs, results[i, action, :], color = action_colors[action], alpha = 0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_indexes, states, actions, rewards, states_prime, dones, importance_weights = agent.replay_memory.sample(\n",
    "    256,\n",
    "    agent.prioritized_replay_beta_function(agent.episode_count, agent.steps)\n",
    ")\n",
    "results = agent.model(states)\n",
    "\n",
    "action_colors=[\"blue\", \"orange\",\"purple\",\"red\"]\n",
    "fig, axes = plt.subplots(nrows = 2, ncols = 2, figsize=(16,9), dpi=300)\n",
    "for action in range(4):\n",
    "    for i in range(1):\n",
    "        axes[action%2, action//2%2].plot(agent.zs, results[i, action, :], color = action_colors[action], alpha = 0.2)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
