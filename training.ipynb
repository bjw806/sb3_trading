{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-04-25T20:45:07.496976Z",
     "iopub.status.busy": "2023-04-25T20:45:07.496697Z",
     "iopub.status.idle": "2023-04-25T20:45:08.612440Z",
     "shell.execute_reply": "2023-04-25T20:45:08.611784Z",
     "shell.execute_reply.started": "2023-04-25T20:45:07.496952Z"
    }
   },
   "outputs": [],
   "source": [
    "import shutil, os\n",
    "if os.path.isdir('rainbow') : shutil.rmtree(\"rainbow\")\n",
    "!git clone https://github.com/ClementPerroud/Rainbow-Agent rainbow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-04-25T20:45:24.690958Z",
     "iopub.status.busy": "2023-04-25T20:45:24.690789Z",
     "iopub.status.idle": "2023-04-25T20:45:27.647431Z",
     "shell.execute_reply": "2023-04-25T20:45:27.646692Z",
     "shell.execute_reply.started": "2023-04-25T20:45:24.690939Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/manager/source/sb3/.venv/lib/python3.11/site-packages/tensorflow_addons/utils/tfa_eol_msg.py:23: UserWarning: \n",
      "\n",
      "TensorFlow Addons (TFA) has ended development and introduction of new features.\n",
      "TFA has entered a minimal maintenance and release mode until a planned end of life in May 2024.\n",
      "Please modify downstream libraries to take dependencies from other repositories in our TensorFlow community (e.g. Keras, Keras-CV, and Keras-NLP). \n",
      "\n",
      "For more information see: https://github.com/tensorflow/addons/issues/2807 \n",
      "\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import gymnasium as gym\n",
    "from sklearn.preprocessing import robust_scale\n",
    "\n",
    "from rainbow.agent import Rainbow\n",
    "from preprocess import preprocess\n",
    "\n",
    "import sys\n",
    "import gym_trading_env\n",
    "import nest_asyncio"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create environments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-04-25T20:45:27.649230Z",
     "iopub.status.busy": "2023-04-25T20:45:27.648870Z",
     "iopub.status.idle": "2023-04-25T20:45:28.576145Z",
     "shell.execute_reply": "2023-04-25T20:45:28.575522Z",
     "shell.execute_reply.started": "2023-04-25T20:45:27.649211Z"
    }
   },
   "outputs": [],
   "source": [
    "def add_features(df):\n",
    "    df[\"feature_close\"] = robust_scale(df[\"close\"].pct_change())\n",
    "    df[\"feature_open\"] = robust_scale(df[\"open\"]/df[\"close\"])\n",
    "    df[\"feature_high\"] = robust_scale(df[\"high\"]/df[\"close\"])\n",
    "    df[\"feature_low\"] = robust_scale(df[\"low\"]/df[\"close\"])\n",
    "    df[\"feature_volume\"] = robust_scale(df[\"volume\"] / df[\"volume\"].rolling(7*24).max())\n",
    "    df.dropna(inplace= True)\n",
    "    return df\n",
    "\n",
    "\n",
    "def reward_function(history):\n",
    "    return np.log(history[\"portfolio_valuation\", -1] / history[\"portfolio_valuation\", -2])\n",
    "\n",
    "def max_drawdown(history):\n",
    "    networth_array = history['portfolio_valuation']\n",
    "    _max_networth = networth_array[0]\n",
    "    _max_drawdown = 0\n",
    "    for networth in networth_array:\n",
    "        if networth > _max_networth:\n",
    "            _max_networth = networth\n",
    "        drawdown = ( networth - _max_networth ) / _max_networth\n",
    "        if drawdown < _max_drawdown:\n",
    "            _max_drawdown = drawdown\n",
    "    return f\"{_max_drawdown*100:5.2f}%\"\n",
    "\n",
    "def make_env(dir):\n",
    "    env = gym.make(\n",
    "        \"MultiDatasetTradingEnv\",\n",
    "        dataset_dir= dir,\n",
    "        preprocess= preprocess,\n",
    "        windows= 15,\n",
    "        positions = [ -1, -0.5, 0, 1, 2],\n",
    "        initial_position = 0,\n",
    "        trading_fees = 0.01/100,\n",
    "        borrow_interest_rate= 0.0003/100,\n",
    "        reward_function = reward_function,\n",
    "        portfolio_initial_value = 1000,\n",
    "        verbose=1,\n",
    "    )\n",
    "    env.unwrapped.add_metric('Position Changes', lambda history : f\"{ 100*np.sum(np.diff(history['position']) != 0)/len(history['position']):5.2f}%\" )\n",
    "    env.unwrapped.add_metric('Max Drawdown', max_drawdown)\n",
    "    return env\n",
    "\n",
    "training_envs = gym.vector.SyncVectorEnv([lambda: make_env(\"./data/train/month/**/*.pkl\") for _ in range(5)])\n",
    "validation_envs = gym.vector.SyncVectorEnv([lambda: make_env(\"./data/test/month/2023/*.pkl\") for _ in range(5)])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-04-25T20:45:28.577112Z",
     "iopub.status.busy": "2023-04-25T20:45:28.576930Z",
     "iopub.status.idle": "2023-04-25T20:45:31.067018Z",
     "shell.execute_reply": "2023-04-25T20:45:31.066482Z",
     "shell.execute_reply.started": "2023-04-25T20:45:28.577095Z"
    }
   },
   "outputs": [],
   "source": [
    "agent = Rainbow(\n",
    "    simultaneous_training_env = 5,\n",
    "    \n",
    "    #Distributional\n",
    "    distributional= True,\n",
    "    v_min= -200,\n",
    "    v_max = 250,\n",
    "    nb_atoms= 51, \n",
    "    # Prioritized Replay\n",
    "    prioritized_replay = False,\n",
    "    prioritized_replay_alpha= 0.5,\n",
    "    prioritized_replay_beta_function = lambda episode, step : min(1, 0.5 + 0.5*step/150_000),\n",
    "    \n",
    "    # General\n",
    "    multi_steps = 3,\n",
    "    nb_states = 14,\n",
    "    nb_actions = 4,\n",
    "    gamma = 0.99,\n",
    "    replay_capacity = 1E8,\n",
    "    tau = 2000,\n",
    "    \n",
    "    # Model\n",
    "    window= 15,\n",
    "    units = [16,16, 16],\n",
    "    dropout= 0.2,\n",
    "    adversarial= True,\n",
    "    noisy= False,\n",
    "    learning_rate = 3*2.5E-4,\n",
    "\n",
    "    batch_size= 128,\n",
    "    train_every = 10,\n",
    "    epsilon_function = lambda episode, step : max(0.001, (1 - 5E-5)** step),\n",
    "    name = \"Rainbow\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-04-25T20:45:31.068198Z",
     "iopub.status.busy": "2023-04-25T20:45:31.067808Z",
     "iopub.status.idle": "2023-04-25T20:45:31.072860Z",
     "shell.execute_reply": "2023-04-25T20:45:31.072377Z",
     "shell.execute_reply.started": "2023-04-25T20:45:31.068180Z"
    }
   },
   "outputs": [],
   "source": [
    "def train(steps = 100_000):\n",
    "    print(\"___________________________________________ TRAINING ___________________________________________\")\n",
    "    if 'obs' not in globals():\n",
    "        global obs\n",
    "        obs, info = training_envs.reset()\n",
    "    for _ in range(steps):\n",
    "        actions = agent.e_greedy_pick_actions(obs)\n",
    "        next_obs, rewards, dones, truncateds, infos = training_envs.step(actions)\n",
    "\n",
    "        agent.store_replays(obs, actions, rewards, next_obs, dones, truncateds)\n",
    "        agent.train()\n",
    "\n",
    "        obs = next_obs\n",
    "\n",
    "def evaluation():\n",
    "    print(\"___________________________________________ VALIDATION ___________________________________________\")\n",
    "    val_obs, info = validation_envs.reset()\n",
    "    check = np.array([False for _ in range(val_obs.shape[0])])\n",
    "    while not np.all(check):\n",
    "        actions = agent.e_greedy_pick_actions(val_obs)\n",
    "        next_obs, rewards, dones, truncateds, infos = validation_envs.step(actions)\n",
    "        val_obs = next_obs\n",
    "        check += dones + truncateds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-04-25T20:45:31.073670Z",
     "iopub.status.busy": "2023-04-25T20:45:31.073517Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "___________________________________________ TRAINING ___________________________________________\n",
      "___________________________________________ VALIDATION ___________________________________________\n",
      "Market Return :  1.34%   |   Portfolio Return : -82.22%   |   Position Changes : 29.76%   |   Max Drawdown : -82.33%   |   \n",
      "Market Return : -9.37%   |   Portfolio Return : -85.61%   |   Position Changes : 29.37%   |   Max Drawdown : -85.75%   |   \n",
      "Market Return : -9.37%   |   Portfolio Return : -84.68%   |   Position Changes : 29.45%   |   Max Drawdown : -84.84%   |   \n",
      "Market Return : -2.43%   |   Portfolio Return : -85.19%   |   Position Changes : 29.67%   |   Max Drawdown : -85.35%   |   \n",
      "Market Return : -7.14%   |   Portfolio Return : -85.30%   |   Position Changes : 29.34%   |   Max Drawdown : -85.39%   |   \n",
      "___________________________________________ TRAINING ___________________________________________\n",
      "Market Return : 36.20%   |   Portfolio Return : -88.47%   |   Position Changes : 46.33%   |   Max Drawdown : -89.19%   |   \n",
      "↳ Env 3 : 000 :    40175   |   00:02:15   |   Epsilon :  13.41%   |   Mean Loss (last 10k) : 3.6756E+00   |   Tot. Rewards :    -2.16   |   Rewards (/1000 steps) :    -0.05   |   Length :  40175\n",
      "Market Return : 40.58%   |   Portfolio Return : -87.84%   |   Position Changes : 44.60%   |   Max Drawdown : -88.19%   |   \n",
      "↳ Env 4 : 000 :    43074   |   00:02:22   |   Epsilon :  11.60%   |   Mean Loss (last 10k) : 3.6563E+00   |   Tot. Rewards :    -2.11   |   Rewards (/1000 steps) :    -0.05   |   Length :  43074\n",
      "Market Return : -14.32%   |   Portfolio Return : -90.83%   |   Position Changes : 44.31%   |   Max Drawdown : -90.98%   |   \n",
      "↳ Env 1 : 000 :    43134   |   00:02:22   |   Epsilon :  11.57%   |   Mean Loss (last 10k) : 3.6559E+00   |   Tot. Rewards :    -2.39   |   Rewards (/1000 steps) :    -0.06   |   Length :  43134\n",
      "Market Return : 13.26%   |   Portfolio Return : -90.86%   |   Position Changes : 43.43%   |   Max Drawdown : -90.99%   |   \n",
      "Market Return : -19.11%   |   Portfolio Return : -91.54%   |   Position Changes : 43.53%   |   Max Drawdown : -91.60%   |   \n",
      "↳ Env 0 : 000 :    44574   |   00:02:26   |   Epsilon :  10.77%   |   Mean Loss (last 10k) : 3.6464E+00   |   Tot. Rewards :    -2.39   |   Rewards (/1000 steps) :    -0.05   |   Length :  44574\n",
      "↳ Env 2 : 000 :    44574   |   00:02:26   |   Epsilon :  10.77%   |   Mean Loss (last 10k) : 3.6464E+00   |   Tot. Rewards :    -2.47   |   Rewards (/1000 steps) :    -0.06   |   Length :  44574\n",
      "___________________________________________ VALIDATION ___________________________________________\n",
      "Market Return :  1.34%   |   Portfolio Return : -28.06%   |   Position Changes :  6.94%   |   Max Drawdown : -29.14%   |   \n",
      "Market Return : 11.84%   |   Portfolio Return : -37.65%   |   Position Changes :  7.21%   |   Max Drawdown : -39.93%   |   \n",
      "Market Return : 11.84%   |   Portfolio Return : -40.24%   |   Position Changes :  6.98%   |   Max Drawdown : -42.39%   |   \n",
      "Market Return :  3.01%   |   Portfolio Return : -32.83%   |   Position Changes :  7.23%   |   Max Drawdown : -33.59%   |   \n",
      "Market Return : -2.43%   |   Portfolio Return : -28.69%   |   Position Changes :  7.34%   |   Max Drawdown : -29.24%   |   \n",
      "___________________________________________ TRAINING ___________________________________________\n",
      "Market Return : 11.48%   |   Portfolio Return : -34.71%   |   Position Changes :  7.78%   |   Max Drawdown : -36.23%   |   \n",
      "↳ Env 1 : 001 :    83389   |   00:05:13   |   Epsilon :  1.55%   |   Mean Loss (last 10k) : 3.3899E+00   |   Tot. Rewards :    -0.43   |   Rewards (/1000 steps) :    -0.01   |   Length :  40255\n",
      "Market Return : -6.44%   |   Portfolio Return : -20.21%   |   Position Changes :  8.16%   |   Max Drawdown : -29.56%   |   \n",
      "↳ Env 3 : 001 :    84750   |   00:05:17   |   Epsilon :  1.44%   |   Mean Loss (last 10k) : 3.3808E+00   |   Tot. Rewards :    -0.23   |   Rewards (/1000 steps) :    -0.01   |   Length :  44575\n",
      "Market Return : 58.65%   |   Portfolio Return : -35.94%   |   Position Changes :  7.25%   |   Max Drawdown : -39.48%   |   \n",
      "↳ Env 4 : 001 :    87049   |   00:05:24   |   Epsilon :  1.29%   |   Mean Loss (last 10k) : 3.3657E+00   |   Tot. Rewards :    -0.45   |   Rewards (/1000 steps) :    -0.01   |   Length :  43975\n",
      "Market Return : -6.47%   |   Portfolio Return : -31.61%   |   Position Changes :  6.99%   |   Max Drawdown : -33.61%   |   \n",
      "↳ Env 0 : 001 :    87709   |   00:05:26   |   Epsilon :  1.25%   |   Mean Loss (last 10k) : 3.3614E+00   |   Tot. Rewards :    -0.38   |   Rewards (/1000 steps) :    -0.01   |   Length :  43135\n",
      "Market Return : 22.73%   |   Portfolio Return : -21.00%   |   Position Changes :  6.83%   |   Max Drawdown : -24.09%   |   \n",
      "↳ Env 2 : 001 :    89149   |   00:05:30   |   Epsilon :  1.16%   |   Mean Loss (last 10k) : 3.3518E+00   |   Tot. Rewards :    -0.24   |   Rewards (/1000 steps) :    -0.01   |   Length :  44575\n",
      "___________________________________________ VALIDATION ___________________________________________\n",
      "Market Return :  1.34%   |   Portfolio Return : -11.03%   |   Position Changes :  1.65%   |   Max Drawdown : -17.97%   |   \n",
      "Market Return :  1.34%   |   Portfolio Return : -10.58%   |   Position Changes :  1.58%   |   Max Drawdown : -18.07%   |   \n",
      "Market Return : 11.84%   |   Portfolio Return : -18.94%   |   Position Changes :  1.62%   |   Max Drawdown : -24.61%   |   \n",
      "Market Return : -7.14%   |   Portfolio Return : -1.56%   |   Position Changes :  1.61%   |   Max Drawdown : -12.78%   |   \n",
      "Market Return : -7.14%   |   Portfolio Return : -2.45%   |   Position Changes :  1.58%   |   Max Drawdown : -12.98%   |   \n",
      "___________________________________________ TRAINING ___________________________________________\n",
      "___________________________________________ VALIDATION ___________________________________________\n",
      "Market Return : 11.84%   |   Portfolio Return : -7.75%   |   Position Changes :  0.41%   |   Max Drawdown : -11.31%   |   \n",
      "Market Return :  3.01%   |   Portfolio Return : -2.35%   |   Position Changes :  0.41%   |   Max Drawdown : -6.39%   |   \n",
      "Market Return :  3.01%   |   Portfolio Return : -2.85%   |   Position Changes :  0.36%   |   Max Drawdown : -6.40%   |   \n",
      "Market Return : 11.84%   |   Portfolio Return : -6.67%   |   Position Changes :  0.38%   |   Max Drawdown : -11.47%   |   \n",
      "Market Return :  3.01%   |   Portfolio Return : -2.61%   |   Position Changes :  0.38%   |   Max Drawdown : -6.46%   |   \n",
      "___________________________________________ TRAINING ___________________________________________\n",
      "Market Return : -17.42%   |   Portfolio Return :  0.67%   |   Position Changes :  1.41%   |   Max Drawdown : -11.60%   |   \n",
      "↳ Env 1 : 002 :   126524   |   00:09:14   |   Epsilon :  0.18%   |   Mean Loss (last 10k) : 2.9304E+00   |   Tot. Rewards :     0.01   |   Rewards (/1000 steps) :     0.00   |   Length :  43135\n",
      "Market Return : -32.76%   |   Portfolio Return : -17.72%   |   Position Changes :  1.31%   |   Max Drawdown : -28.13%   |   \n",
      "↳ Env 3 : 002 :   129325   |   00:09:23   |   Epsilon :  0.16%   |   Mean Loss (last 10k) : 2.8935E+00   |   Tot. Rewards :    -0.20   |   Rewards (/1000 steps) :    -0.00   |   Length :  44575\n",
      "Market Return : -3.51%   |   Portfolio Return : 28.70%   |   Position Changes :  1.28%   |   Max Drawdown : -8.95%   |   \n",
      "↳ Env 4 : 002 :   130184   |   00:09:26   |   Epsilon :  0.15%   |   Mean Loss (last 10k) : 2.8822E+00   |   Tot. Rewards :     0.25   |   Rewards (/1000 steps) :     0.01   |   Length :  43135\n",
      "Market Return : -9.69%   |   Portfolio Return :  5.26%   |   Position Changes :  1.21%   |   Max Drawdown : -27.11%   |   \n",
      "↳ Env 0 : 002 :   132284   |   00:09:32   |   Epsilon :  0.13%   |   Mean Loss (last 10k) : 2.8545E+00   |   Tot. Rewards :     0.05   |   Rewards (/1000 steps) :     0.00   |   Length :  44575\n",
      "Market Return : 21.44%   |   Portfolio Return : -6.50%   |   Position Changes :  1.17%   |   Max Drawdown : -14.02%   |   \n",
      "↳ Env 2 : 002 :   133724   |   00:09:37   |   Epsilon :  0.12%   |   Mean Loss (last 10k) : 2.8356E+00   |   Tot. Rewards :    -0.07   |   Rewards (/1000 steps) :    -0.00   |   Length :  44575\n",
      "___________________________________________ VALIDATION ___________________________________________\n",
      "Market Return :  3.01%   |   Portfolio Return : -2.43%   |   Position Changes :  0.13%   |   Max Drawdown : -6.43%   |   \n",
      "Market Return :  3.01%   |   Portfolio Return : -2.29%   |   Position Changes :  0.12%   |   Max Drawdown : -6.36%   |   \n",
      "Market Return :  3.01%   |   Portfolio Return : -2.13%   |   Position Changes :  0.13%   |   Max Drawdown : -6.38%   |   \n",
      "Market Return : 11.84%   |   Portfolio Return : -6.21%   |   Position Changes :  0.12%   |   Max Drawdown : -11.55%   |   \n",
      "Market Return : -7.14%   |   Portfolio Return :  2.76%   |   Position Changes :  0.15%   |   Max Drawdown : -5.28%   |   \n",
      "___________________________________________ TRAINING ___________________________________________\n"
     ]
    }
   ],
   "source": [
    "while True:\n",
    "    train(steps = 30_000)\n",
    "    evaluation()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dill, pickle\n",
    "#agent.model = None\n",
    "#agent.target_model = None\n",
    "#agent.replay_memory = None\n",
    "\n",
    "with open(\"test.pkl\", \"wb\") as file:\n",
    "    dill.dump(agent, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_indexes, states, actions, rewards, states_prime, dones, importance_weights = agent.replay_memory.sample(\n",
    "    256,\n",
    "    agent.prioritized_replay_beta_function(agent.episode_count, agent.steps)\n",
    ")\n",
    "results = agent.model(states)\n",
    "\n",
    "action_colors=[\"blue\", \"orange\",\"purple\",\"red\"]\n",
    "fig, axes = plt.subplots(nrows = 2, ncols = 2, figsize=(16,9), dpi=300)\n",
    "for action in range(4):\n",
    "    for i in range(256):\n",
    "        axes[action%2, action//2%2].plot(agent.zs, results[i, action, :], color = action_colors[action], alpha = 0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_indexes, states, actions, rewards, states_prime, dones, importance_weights = agent.replay_memory.sample(\n",
    "    256,\n",
    "    agent.prioritized_replay_beta_function(agent.episode_count, agent.steps)\n",
    ")\n",
    "results = agent.model(states)\n",
    "\n",
    "action_colors=[\"blue\", \"orange\",\"purple\",\"red\"]\n",
    "fig, axes = plt.subplots(nrows = 2, ncols = 2, figsize=(16,9), dpi=300)\n",
    "for action in range(4):\n",
    "    for i in range(1):\n",
    "        axes[action%2, action//2%2].plot(agent.zs, results[i, action, :], color = action_colors[action], alpha = 0.2)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
